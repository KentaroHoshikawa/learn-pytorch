{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorchによるpix2pixの写経\n",
    "## 参照URL\n",
    "https://github.com/GINK03/pytorch-pix2pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import  Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重みの初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1 or classname.find('InstanceNorm2d') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正規化層の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_norm_layer(norm_type):\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "    elif norm_type == 'instance':\n",
    "        norm_layer = nn.InstanceNorm2d\n",
    "    else:\n",
    "        print('normalization layer [%s] is not found' % norm_type)\n",
    "    \n",
    "    return norm_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generatorの生成関数\n",
    "`ResnetGenerator`のラッパー関数という印象を受ける。特にやっていることは、正規化層の選択と、GPUを利用するならこの時点で送るといったところか。\n",
    "\n",
    "しかし、`cuda()`するのと`to(device)`するのはどっちが主流なのか？←https://qiita.com/vintersnow/items/91545c27e2003f62ebc4　によると`to(device)`はバージョンアップで使えるようになった表現らしい。これで、条件分岐なしでCPU・GPU両方に向けたコードが書ける。\n",
    "\n",
    "\n",
    "因みに`torch.nn.Module#apply(fn)`\n",
    "は子の`nn.Module`全てに`fn`を適用するメソッドである。つまり、`netG.apply(weights_init)`で全ての層のパラメータを初期化しているわけである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_G(input_nc, output_nc, ngf, norm='batch', use_dropout=False, gpu_ids=[]):\n",
    "    netG = None\n",
    "    use_gpu = len(gpu_ids) > 0\n",
    "    norm_layer = get_norm_layer(norm_type=norm)\n",
    "    \n",
    "    if use_gpu:\n",
    "        assert(torch.cuda.is_available())\n",
    "    \n",
    "    netG = ResnetGenerator(input_nc, output_nc, ngf, \n",
    "                           norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9, gpu_ids=gpu_ids)\n",
    "    \n",
    "    if len(gpu_ids) > 0:\n",
    "        netG.cuda()\n",
    "        \n",
    "    netG.apply(weights_init)\n",
    "    return netG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminatorの生成関数\n",
    "`define_G()`と同様に`NLayerDiscriminator`のラッパーという感じだ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_D(input_nc, ndf, norm='batch', use_sigmoid=False, gpu_ids=[]):\n",
    "    netD = None\n",
    "    use_gpu = len(gpu_ids) > 0\n",
    "    norm_layer = get_norm_layer(norm_type=norm)\n",
    "    \n",
    "    if use_gpu:\n",
    "        assert(torch.cuda.is_available())\n",
    "        \n",
    "    netD = NLayerDiscriminator(input_nc, ndf, n_layers=3, \n",
    "                               norm_layer=norm_layer, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)\n",
    "    \n",
    "    if use_gpu:\n",
    "        netD.cuda()\n",
    "    \n",
    "    netD.apply(weights_init)\n",
    "    return netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_network(net):\n",
    "    num_params = 0\n",
    "    for param in net.parameters():\n",
    "        num_params += param.numel()\n",
    "    \n",
    "    print(net)\n",
    "    print('Total number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class GANLoss\n",
    "githubによると\n",
    "```\n",
    "LSGANや通常のGANで用いられるGANの損失を定義する\n",
    "```\n",
    "とある。\n",
    "\n",
    "使われ方を見たり、中身を見たらわかるだろう（希望的観測）。\n",
    "\n",
    "真偽の数値を決めておいて、使用する損失関数を状況に合わせて変えるためのAdapterといってもよいのだろうか？\n",
    "\n",
    "### numel()\n",
    "\n",
    "### Binary Cross Entropy Loss (BCE Loss)\n",
    "$N$をバッチサイズとして、\n",
    "\\begin{align*}\n",
    "l(x, y ) &= L = \\lbrace l_1, l_2, \\dots, l_N \\rbrace ^T \\\\\n",
    "l_n &= -w_n \\left(  y_n \\cdot \\log x_n + (1 - y_n)\\cdot \\log(1 - x_n) \\right) \n",
    "\\end{align*}\n",
    "\n",
    "### Mean Squared Error Loss (MSE Loss)\n",
    "\\begin{align*}\n",
    "l(x, y ) &= L = \\lbrace l_1, l_2, \\dots, l_N \\rbrace ^T \\\\\n",
    "l_n &= (x_n - y_n)^2 \n",
    "\\end{align*}\n",
    "2乗距離。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n",
    "                 tensor=torch.FloatTensor):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.real_label = target_real_label\n",
    "        self.fake_label = target_fake_label\n",
    "        self.real_label_var = self.fake_label_var = None\n",
    "        self.Tensor = tensor\n",
    "        if use_lsgan:\n",
    "            self.loss = nn.MSELoss()\n",
    "        else:\n",
    "            self.loss = nn.BCELoss()\n",
    "        \n",
    "    def get_target_tensor(self, input, target_is_real):\n",
    "        target_tensor = None\n",
    "        if target_is_real:\n",
    "            create_label = ((self.real_label_var is None) or\n",
    "                            (self.real_label_var.numel() != input.numel()))\n",
    "            if create_label:\n",
    "                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n",
    "                self.real_label_var = Variable(real_tensor, requires_grad = False)\n",
    "            target_tensor  = self.fake_label_var\n",
    "        else:\n",
    "            create_label = ((self.fake_label_var is None) or\n",
    "                            (self.fake_label_var.numel() != input.numel()))\n",
    "            if create_label:\n",
    "                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n",
    "                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n",
    "            target_tensor = fake_tensor\n",
    "            \n",
    "        return target_tensor\n",
    "    \n",
    "    # get_target_tensorのラッパ\n",
    "    # forward()に対する__call__()のようなものだろう\n",
    "    def __call__(self,input, target_is_real):\n",
    "        target_tensor = self.get_target_tensor(input, target_is_real)\n",
    "        return self.loss(input, target_tensor.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class ResnetGenerator\n",
    "本丸その１：生成器。\n",
    "\n",
    "### torch.cuda.FloatTensor\n",
    "`cuda()`や`to(device)`でcuda対応のGPUに転送されたテンソルのクラス。現在のバージョンではコンストラクタの様式が変わった(`torch.Tensor(・, dtype=・, device=・)`)ので、型がどうなっているのか不思議だったがisinstanceしたところ、`dtype`, `device`により型が変わるようだ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResnetGenerator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, \n",
    "                 use_dropout=False, n_blocks=6, gpu_ids=[]):\n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResnetGenerator, self).__init__\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.ngf = ngf\n",
    "        self.gpu_ids = gpu_ids\n",
    "        \n",
    "        model = [nn.Conv2d(input_nc, ngf, kernel_size=7, padding=3),\n",
    "                 norm_layer(ngf, affine=True),\n",
    "                 nn.ReLU(True)]\n",
    "        \n",
    "        n_downsampling = 2\n",
    "        # チャンネル数を増やす\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**i\n",
    "            model += [\n",
    "                nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, \n",
    "                          stride=2, padding=1),\n",
    "                norm_layer(ngf * mult * 2, affine=True),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "            \n",
    "        \n",
    "        mult = 2**n_downsampling\n",
    "        # 残余ネットワークの層を追加\n",
    "        for i in range(n_blocks):\n",
    "            model += [ResNetBlock(ngf * mult, 'zero', norm_layer=norm_layer, use_dropout=use_dropout)]\n",
    "        \n",
    "        # チャンネル数を小さくする\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**(n_downsampling - 1)\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                   kernel_size=3, stride=2,\n",
    "                                   padding=1, output_padding=1),\n",
    "                norm_layer(int(ngf * mult / 2), affine=True),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "        \n",
    "        # 出力層用に形を整える\n",
    "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=3)]\n",
    "        model += [nn.Tanh()]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor):\n",
    "            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n",
    "        else:\n",
    "            return self.model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class ResnetBlock\n",
    "親玉。一番よくわからない。https://deepage.net/deep_learning/2016/11/30/resnet.html でちゃんと勉強しよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, padding_type, norm_layer, use_dropout):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout)\n",
    "        \n",
    "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout):\n",
    "        conv_block = []\n",
    "        p = 0\n",
    "        assert(padding_type == 'zero')\n",
    "        p = 1\n",
    "        \n",
    "        conv_block += [\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n",
    "            norm_layer(dim, affine=True),\n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "        \n",
    "        if use_dropout:\n",
    "            conv_block  += [nn.Dropout(0.5)]\n",
    "        \n",
    "        conv_block += [\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n",
    "            norm_layer(dim,affine(True))\n",
    "        ]\n",
    "        return nn.Sequential(*conv_block)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class NLayerDiscriminator\n",
    "本丸その２。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NLayerDiscriminator(nn.Module):\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, \n",
    "                 norm_layer=nn.BatchNorm2d, use_sigmoid=False, gpu_ids=[]):\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        self.gpu_ids = gpu_ids\n",
    "        \n",
    "        kw = 4\n",
    "        padw = int(np.ceil((kw-1)/2))\n",
    "        seq = [\n",
    "            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "        \n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        \n",
    "        for n in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2**n, 8)\n",
    "            seq += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw),\n",
    "                norm_layer(ndf, * nf_mult, affine=True),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "        \n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2**n_layers, 8)\n",
    "        seq += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw),\n",
    "            norm_layer(ndf, * nf_mult, affine=True),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "        \n",
    "        seq += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n",
    "        \n",
    "        if use_sigmoid:\n",
    "            seq += [nn.Sigmoid()]\n",
    "        \n",
    "        self.model = nn.Sequential(*seq)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # 0.4以降はTensorが計算グラフを持てるので、こうする必要があるかは調査が必要\n",
    "        if len(self.gpu_ids) and isinstance(input.data, torch.cuda.FloatTensor):\n",
    "            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n",
    "        else:\n",
    "            return self.model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### パラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_nc = 3\n",
    "output_nc = 2\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Adam用\n",
    "lr = 0.00015\n",
    "beta = (0.5, 0.999)\n",
    "\n",
    "# L1用\n",
    "lamb = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-27a2a2389bbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_G\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_nc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_nc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnetD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_nc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_nc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_sigmoid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-204cec0c4738>\u001b[0m in \u001b[0;36mdefine_G\u001b[0;34m(input_nc, output_nc, ngf, norm, use_dropout, gpu_ids)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     netG = ResnetGenerator(input_nc, output_nc, ngf, \n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "netG = define_G(input_nc, output_nc, ngf, norm='batch', use_dropout=False, gpu_ids=[0])\n",
    "netD = define_D(input_nc, output_nc, ndf, norm='batch', use_sigmoid=False, gpu_ids=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数\n",
    "\n",
    "### nn.L1Loss\n",
    "\\begin{align*}\n",
    "l(x,y) &= L = \\lbrace l_1, \\dots, l_N \\rbrace^\\top \\\\\n",
    "l_n &= |x_n - y_n|\n",
    "\\end{align*}\n",
    "要するに、差の絶対値\n",
    "### nn.MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterionGAN = GANLoss()\n",
    "criterionL1 = nn.L1Loss()\n",
    "criterionMSE = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最適化スキーム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'netG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4bcbdc3dd2cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizerG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizerD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'netG' is not defined"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "optimizerG = optim.Adam(netG.parameters(), lr, beta)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 変数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_a = torch.Tensor(batch_size, input_nc, 256, 256)\n",
    "real_b = torch.Tensor(batch_size, output_nc, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## デバイス(CPU or GPU)の設定 & 変数化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'netG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c326ec2b9d8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcriterionGAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterionL1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterionMSE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m real_a, real_b = map(lambda x: x.to(device), \n\u001b[0;32m----> 6\u001b[0;31m                      [netG, netD, criterionGAN, criterionL1, criterionMSE, real_a, real_b])\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mreal_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'netG' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "netG, netD, \\\n",
    "criterionGAN, criterionL1, criterionMSE, \\\n",
    "real_a, real_b = map(lambda x: x.to(device), \n",
    "                     [netG, netD, criterionGAN, criterionL1, criterionMSE, real_a, real_b])\n",
    "\n",
    "real_a = Variable(real_a)\n",
    "real_b = Variable(real_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "\n",
    "### torch.cat(seq, dim=0, out=None) → Tensor\n",
    "与えられた`Tensor`の列`seq`を次元`dim`の軸で連結する。\n",
    "```py\n",
    ">>> x = torch.randn(2, 3)\n",
    ">>> x\n",
    "tensor([[ 0.6580, -1.0969, -0.4614],\n",
    "        [-0.1034, -0.5790,  0.1497]])\n",
    ">>> torch.cat((x, x, x), 0)\n",
    "tensor([[ 0.6580, -1.0969, -0.4614],\n",
    "        [-0.1034, -0.5790,  0.1497],\n",
    "        [ 0.6580, -1.0969, -0.4614],\n",
    "        [-0.1034, -0.5790,  0.1497],\n",
    "        [ 0.6580, -1.0969, -0.4614],\n",
    "        [-0.1034, -0.5790,  0.1497]])\n",
    ">>> torch.cat((x, x, x), 1)\n",
    "tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,\n",
    "         -1.0969, -0.4614],\n",
    "        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,\n",
    "         -0.5790,  0.1497]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    for iteration, batch in enumerate(training_data_loader, 1):\n",
    "        # forward\n",
    "        real_a_cpu, real_b_cpu = batch[0:2]\n",
    "        \n",
    "        # 本物画像\n",
    "        real_a.data.resize_(real_a_cpu.size()).copy_(real_a_cpu)\n",
    "        real_b.data.resize_(real_b_cpu.size()).copy_(real_b_cpu)\n",
    "        # 生成器の作った偽物画像\n",
    "        fake_b = netG(real_a)\n",
    "        \n",
    "        #-----------------------------------------\n",
    "        # ●判別器の更新\n",
    "        # max log(D(x,y)) + log(1 - D(x, G(x)))\n",
    "        #-----------------------------------------\n",
    "        optimizerD.zero_grad()\n",
    "        \n",
    "        # 偽物で学習\n",
    "        fake_ab = torch.cat((real_a, fake_b), 1)\n",
    "        pred_fake = netD.forward(fake_ab.detach())   \n",
    "        # ↑なんで__call__じゃないんだ？\n",
    "        #   `detach`するのは勾配がGeneratorに伝わらないようにするためだったか？\n",
    "        loss_d_fake = criterionGAN(pred_fake, target_is_real=False)\n",
    "        \n",
    "        # 本物で学習\n",
    "        real_ab = torch.cat((real_a, real_b), 1)\n",
    "        pred_real = netD.forward(real_ab)\n",
    "        loss_d_real = criterionGAN(pred_real, target_is_real=True)\n",
    "        \n",
    "        # 損失を合算\n",
    "        loss_d = (loss_d_fake, loss_d_real) * 0.5\n",
    "        loss_d.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        #-----------------------------------------\n",
    "        # ●生成器の更新\n",
    "        # max log(D(x,G(x))) + L1(y, G(x))\n",
    "        #-----------------------------------------\n",
    "        optimizerG.zero_grad()\n",
    "        # 偽物で騙しにかかる (第１項)\n",
    "        fake_ab = torch.cat((real_a, fake_b), 1)\n",
    "        pred_fake = netD.forward(fake_ab)\n",
    "        loss_g_gan = criterionGAN(pred_fake, target_is_real=True)\n",
    "        \n",
    "        # どれだけ本物に近かったか（第２項）\n",
    "        loss_g_l1 = criterionL1(fake_b, real_b) * lamb\n",
    "        loss_g = loss_g_gan + loss_g_l1\n",
    "        loss_g.backward()\n",
    "        optimizerG.step()\n",
    "        print(\"===> Epoch[{}]({}/{}): Loss_D: {:.4f} Loss_G: {:.4f}\".format(\n",
    "                epoch, iteration, len(training_data_loader), loss_d.data[0], loss_g.data[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
